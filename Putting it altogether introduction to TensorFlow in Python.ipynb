# Putting it altogether: Introduction to TensorFlow in Python: set constant, performing element-wise multiplication, reshaping tensors
from tensorflow import constant                                                                                                             # Import constant from TensorFlow
credit_constant = constant(credit_numpy)                                                                                                    # Convert the credit_numpy array into a tensorflow constant
print('\n The datatype is:', credit_constant.dtype)                                                                                         # Print constant datatype
print('\n The shape is:', credit_constant.shape)                                                                                            # Print constant shape
A1 = constant([1, 2, 3, 4])                                                                                                                 # Define tensors A1 and A23 as constants
A23 = constant([[1, 2, 3], [1, 6, 4]])
B1 = ones_like(A1)                                                                                                                          # Define B1 and B23 to have the correct shape
B23 = ones_like(A23)
C1 = multiply(A1,B1)                                                                                                                        # Perform element-wise multiplication
C23 = multiply(A23,B23)
print('\n C1: {}'.format(C1.numpy()))                                                                                                       # Print the tensors C1 and C23
print('\n C23: {}'.format(C23.numpy()))
features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])                                                                                   # Define features, params, and bill as constants
params = constant([[1000], [150]])
bill = constant([[3913], [2682], [8617], [64400]])
billpred = matmul(features,params)                                                                                                          # Compute billpred using features and params
error = bill - billpred                                                                                                                     # Compute and print the error
print(error.numpy())
gray_vector = reshape(gray_tensor, (784, 1))                                                                                                # Reshape the grayscale image tensor into a vector
color_vector = reshape(color_tensor, (2352, 1))                                                                                             # Reshape the color image tensor into a vector

# Putting it altogether: Optimizing with gradients
def compute_gradient(x0):  	
	x = Variable(x0)                                                                                                                          # Define x as a variable with an initial value of x0
	with GradientTape() as tape:
		tape.watch(x)        
		y = multiply(x,x)                                                                                                                       # Define y using the multiply operation    
	return tape.gradient(y, x).numpy()                                                                                                        # Return the gradient of y with respect to x
print(compute_gradient(-1.0))                                                                                                               # Compute and print gradients at x = -1, 1, and 0
print(compute_gradient(1.0))
print(compute_gradient(0.0))

# Putting it altogether: Working with image data
model = reshape(model, (3, 1))                                                                                                              # Reshape model from a 1x3 to a 3x1 tensor
output = matmul(letter, model)                                                                                                              # Multiply letter by model
prediction = reduce_sum(output)                                                                                                             # Sum over output and print prediction using the numpy method
print(prediction.numpy())





# Putting it altogether: Linear models: load data, set the data type, compute loss use two different functions
import pandas as pd                                                                                                                         # Import pandas under the alias pd
import numpy as np                                                                                                                          # Import numpy and tensorflow with their standard aliases
import tensorflow as tf
from tensorflow import keras                                                                                                                # Import the keras module from tensorflow
data_path = 'kc_house_data.csv'                                                                                                             # Assign the path to a string variable named data_path
housing = pd.read_csv(data_path)                                                                                                            # Load the dataset as a dataframe named housing
print(housing['price'])                                                                                                                     # Print the price column of housing
price = np.array(housing['price'], np.float32)                                                                                              # Use a numpy array to define price as a 32-bit float
waterfront = tf.cast(housing['waterfront'], tf.bool)                                                                                        # Define waterfront as a Boolean using cast
print(price)                                                                                                                                # Print price and waterfront
print(waterfront)
loss = keras.losses.mse(price, predictions)                                                                                                 # Compute the mean absolute error (mse)
loss = keras.losses.mae(price, predictions)                                                                                                 # Compute the mean absolute error (mae)
print(loss.numpy())                                                                                                                         # Print the loss

# Putting it altogether: Set up a Multiple linear regression with mse loss function and train a linear model
def linear_regression(params, feature1 = size_log, feature2 = bedrooms):                                                                    # Define the linear regression model
	return params[0] + feature1*params[1] + feature2*params[2]
def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):                                                   # Define the loss function	
	predictions = linear_regression(params, feature1, feature2)                                                                               # Set the predicted values	
	return keras.losses.mae(targets, predictions)                                                                                             # Use the mean absolute error loss
opt = keras.optimizers.Adam()                                                                                                               # Define the optimize operation
for j in range(10):
	opt.minimize(lambda: loss_function(params), var_list=[params])                                                                            # Perform minimization and print trainable variables
	if j % 10 == 0:                                                                                                                           # Print every 10th value of the loss
		print(loss_function(params).numpy())
print_results(params)
# Training a linear model in batches
for batch in pd.read_csv('kc_house_data.csv', chunksize=100):                                                                               # Load data in batches
	size_batch = np.array(batch['sqft_lot'], np.float32)	
	price_batch = np.array(batch['price'], np.float32)                                                                                        # Extract the price values for the current batch	
	opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])                               # Complete the loss, fill in the variable list, and minimize
print(intercept.numpy(), slope.numpy())                                                                                                     # Print trained parameters





# Putting it altogether: The linear algebra of dense layers: The low-level approach with multiple examples
bias1 = Variable(1.0)                                                                                                                       # Initialize bias1
weights1 = Variable(ones((3, 2)))                                                                                                           # Initialize weights1 as 3x2 variable of ones
product1 = matmul(borrower_features,weights1)                                                                                               # Perform matrix multiplication of borrower_features and weights1
dense1 = keras.activations.sigmoid(product1 + bias1)                                                                                        # Apply sigmoid activation function to product1 + bias1
print("\n dense1's output shape: {}".format(dense1.shape))                                                                                  # Print shape of dense1
print('\n shape of borrower_features: ', borrower_features.shape)                                                                           # Print the shapes of borrower_features, weights1, bias1, and dense1
print('\n shape of weights1: ', weights1.shape)
print('\n shape of bias1: ', bias1.shape)
print('\n shape of dense1: ', dense1.shape)
bias2 = Variable(1.0)                                                                                                                       # Initialize bias2 and weights2
weights2 = Variable(ones((2, 1)))
product2 = matmul(dense1,weights2)                                                                                                          # Perform matrix multiplication of dense1 and weights2
prediction = keras.activations.sigmoid(product2 + bias2)                                                                                    # Apply activation to product2 + bias2 and print the prediction
print('\n prediction: {}'.format(prediction.numpy()[0,0]))
print('\n actual: 1')

# Putting it altogether: Binary classification problems
inputs = constant(bill_amounts,float32)                                                                                                     # Construct input layer from features
dense1 = keras.layers.Dense(3, activation='relu')(inputs)                                                                                   # Define first dense layer
dense2 = keras.layers.Dense(2, activation='relu')(dense1)                                                                                   # Define second dense layer
outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)                                                                               # Define output layer
error = default[:5] - outputs.numpy()[:5]                                                                                                   # Print error for first five examples
print(error)
print('\n shape of dense1: ', dense1.shape)                                                                                                 # Print the shapes of dense1, dense2, and predictions
print('\n shape of dense2: ', dense2.shape)
print('\n shape of predictions: ', outputs.shape)
# Multiclass classification problems
outputs = keras.layers.Dense(6, activation='softmax')(dense2)                                                                               # Define output layer
print(outputs.numpy()[:5])                                                                                                                  # Print first five predictions

#  Putting it altogether: Avoiding local minima
x_1 = Variable(0.05,float32)                                                                                                                # Initialize x_1 and x_2
x_2 = Variable(0.05,float32)
opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)                                                                         # Define the optimization operation for opt_1 and opt_2
opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)
for j in range(100):
	opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])
	opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])                                                                                # Define the minimization operation for opt_2
print(x_1.numpy(), x_2.numpy())                                                                                                             # Print x_1 and x_2 as numpy arrays

# Putting it altogether
# 1. Initialization in TensorFlow
w1 = Variable(random.normal([23, 7]))                                                                                                       # Define the layer 1 weights
b1 = Variable(ones([7]))                                                                                                                    # Initialize the layer 1 bias
w2 = Variable(random.normal([7, 1]))                                                                                                        # Define the layer 2 weights
b2 = Variable(0.0)                                                                                                                          # Define the layer 2 bias
# 2. Defining the model and loss function
def model(w1, b1, w2, b2, features = borrower_features):                                                                                    # Define the model	
	layer1 = keras.activations.relu(matmul(features, w1) + b1)                                                                                # Apply relu activation functions to layer 1   
	dropout = keras.layers.Dropout(0.25)(layer1)                                                                                              # Apply dropout
	return keras.activations.sigmoid(matmul(dropout, w2) + b2)
def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):                                                         # Define the loss function
	predictions = model(w1, b1, w2, b2)	
	return keras.losses.binary_crossentropy(targets, predictions)                                                                             # Pass targets and predictions to the cross entropy loss
# 3. Training neural networks with TensorFlow
for j in range(100):                                                                                                                        # Train the model    
	opt.minimize(lambda: loss_function(w1, b1, w2, b2),                                                                                       # Complete the optimizer
                 var_list=[w1, b1, w2, b2])
model_predictions = model(w1, b1, w2, b2, test_features)                                                                                    # Make predictions with model
confusion_matrix(test_targets, model_predictions)                                                                                           # Construct the confusion matrix





# Putting it altogether: The sequential model in Keras
model = keras.Sequential()                                                                                                                  # Define a Keras sequential model
model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))                                                                 # Define the first dense layer
model.add(keras.layers.Dropout(0.25))                                                                                                       # Apply dropout to the first layer's output
model.add(keras.layers.Dense(8, activation='relu'))                                                                                         # Define the second dense layer
model.add(keras.layers.Dense(4, activation='softmax'))                                                                                      # Define the output layer
model.compile(optimizer=keras.optimizers.Adam(lr=0.001),                                                                                    # Finish the model compilation
              loss='categorical_crossentropy', metrics=['accuracy'])
print(model.summary())                                                                                                                      # Print a model summary
model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)                                                    # Add the number of epochs and the validation split
# Evaluating models
small_train = small_model.evaluate(train_features, train_labels)                                                                            # Evaluate the small model using the train data
small_test = small_model.evaluate(test_features, test_labels)                                                                               # Evaluate the small model using the test data
large_train = large_model.evaluate(train_features, train_labels)                                                                            # Evaluate the large model using the train data
large_test = large_model.evaluate(test_features, test_labels)                                                                               # Evaluate the large model using the test data
print('\n Small - Train: {}, Test: {}'.format(small_train, small_test))                                                                     # Print losses
print('Large - Train: {}, Test: {}'.format(large_train, large_test))

# Putting it altogether: Defining a multiple input model
m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)                                                                         # For model 1, pass the input layer to layer 1 and layer 1 to layer 2
m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)
m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)                                                                            # For model 2, pass the input layer to layer 1 and layer 1 to layer 2
m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)
merged = keras.layers.add([m1_layer2, m2_layer2])                                                                                           # Merge model outputs and define a functional model
model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)
print(model.summary())                                                                                                                      # Print a model summary

# Putting it altogether: Preparing to train with Estimators
bedrooms = feature_column.numeric_column("bedrooms")                                                                                        # Define feature columns for bedrooms and bathrooms
bathrooms = feature_column.numeric_column("bathrooms")
feature_list = [bedrooms, bathrooms]                                                                                                        # Define the list of feature columns
def input_fn():	
	labels = np.array(housing['price'])                                                                                                       # Define the labels	
	features = {'bedrooms':np.array(housing['bedrooms']),                                                                                     # Define the features
                'bathrooms':np.array(housing['bathrooms'])}
	return features, labels
# Defining Estimators
model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])                                                            # Define the model and set the number of steps
model.train(input_fn, steps=1)
model = estimator.LinearRegressor(feature_columns=feature_list)                                                                             # Define the model and set the number of steps
model.train(input_fn, steps=2)
