# Introducing Keras
# Hello nets!
from keras.models import Sequential# Import the Sequential model and Dense layer
from keras.layers import Dense
model = Sequential()# Create a Sequential model
model.add(Dense(10, input_shape=(2,), activation="relu"))# Add an input layer and a hidden layer with 10 neurons
model.add(Dense(1))# Add a 1-neuron output layer
model.summary()# Summarise your model

# Instantiate a new Sequential model
model = Sequential()
model.add(Dense(5, input_shape=(3,), activation="relu"))# Add a Dense layer with five neurons and three inputs
model.add(Dense(1))# Add a final Dense layer with one neuron and no activation
model.summary()# Summarize your model

# Build as shown!
from keras.models import Sequential
from keras.layers import Dense
model = Sequential()# Instantiate a Sequential model
model.add(Dense(3,input_shape=(2,)))# Build the input and hidden layer
model.add(Dense(1))# Add the ouput layer

# Specifying a model
model = Sequential()# Instantiate a Sequential model
model.add(Dense(50, input_shape=(1,), activation='relu'))# Add a Dense layer with 50 neurons and an input of 1 neuron
model.add(Dense(50,activation='relu'))# Add two Dense layers with 50 neurons and relu activation
model.add(Dense(50,activation='relu'))
model.add(Dense(1))# End your model with a Dense layer and no activation

# Training
model.compile(optimizer = 'adam', loss = 'mse')# Compile your model
print("Training started..., this can take a while:")
model.fit(time_steps,y_positions, epochs = 30)# Fit your model on your data for 30 epochs
print("Final loss value:",model.evaluate(time_steps,y_positions))# Evaluate your model 

# Predicting the orbit!
twenty_min_orbit = model.predict(np.arange(-10, 10))# Predict the twenty minutes orbit
plot_orbit(twenty_min_orbit)# Plot the twenty minute orbit 
eighty_min_orbit = model.predict(np.arange(-40, 41))# Predict the eighty minute orbit
plot_orbit(eighty_min_orbit)# Plot the eighty minute orbit 





# Going Deeper
# Exploring dollar bills
import seaborn as sns# Import seaborn
sns.pairplot(banknotes, hue='class') # Use pairplot and set the hue to be our class column
plt.show()# Show the plot
print('Dataset stats: \n', banknotes.describe())# Describe the data
print('Observations per class: \n', banknotes['class'].value_counts())# Count the number of observations per class

# A binary classification model
from keras.models import Sequential# Import the sequential model and dense layer
from keras.layers import Dense
model = Sequential()# Create a sequential model
model.add(Dense(1, input_shape=(4,), activation='sigmoid'))# Add a dense layer 
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])# Compile your model
model.summary()# Display a summary of your model

# Is this dollar bill fake ?
model.fit(X_train, y_train, epochs = 20)# Train your model for 20 epochs
accuracy = model.evaluate(X_test, y_test)[1]# Evaluate your model accuracy on the test set
print('Accuracy:', accuracy)# Print accuracy

# A multi-class model
model = Sequential()# Instantiate a sequential model
model.add(Dense(128, input_shape=(2,), activation='relu'))# Add 3 dense layers of 128, 64 and 32 neurons each
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu')) 
model.add(Dense(4, activation='softmax')) # Add a dense layer with as many neurons as competitors
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])# Compile your model using categorical_crossentropy loss

# Prepare your dataset
darts.competitor = pd.Categorical(darts.competitor)# Transform into a categorical variable
darts.competitor = darts.competitor.cat.codes # Assign a number to each category (label encoding)
from keras.utils import to_categorical# Import to_categorical from keras utils module
coordinates = darts.drop(['competitor'], axis=1)
competitors = to_categorical(darts.competitor)# Use to_categorical on your labels
print('One-hot encoded competitors: \n',competitors)# Now print the one-hot encoded labels

# Training on dart throwers
model.fit(coord_train,competitors_train,epochs=200)# Fit your model to the training data for 200 epochs
accuracy = model.evaluate(coord_test, competitors_test)[1]# Evaluate your model accuracy on the test data
print('Accuracy:', accuracy)# Print accuracy

# Softmax predictions
preds = model.predict(coords_small_test)# Predict on coords_small_test
print("{:45} | {}".format('Raw Model Predictions','True labels'))# Print preds vs true values
for i,pred in enumerate(preds):
  print("{} | {}".format(pred,competitors_small_test[i]))
preds_chosen = [np.argmax(2) for pred in preds]# Extract the position of highest probability from each pred vector
print("{:10} | {}".format('Rounded Model Predictions','True labels'))# Print preds vs true values
for i,pred in enumerate(preds_chosen):
  print("{:25} | {}".format(pred,competitors_small_test[i]))

# An irrigation machine
model = Sequential()# Instantiate a Sequential model
model.add(Dense(64,input_shape=(20,),activation='relu'))# Add a hidden layer of 64 neurons and a 20 neuron's input
model.add(Dense(3,activation='sigmoid'))# Add an output layer of 3 neurons with sigmoid activation
model.compile(optimizer='adam',loss = 'binary_crossentropy',metrics=['accuracy'])# Compile your model with binary crossentropy loss
model.summary()

# Training with multiple labels
model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)# Train for 100 epochs using a validation split of 0.2
preds = model.predict(sensors_test)# Predict on sensors_test and round up the predictions
preds_rounded = np.round(preds)
print('Rounded Predictions: \n', preds_rounded)# Print rounded preds
accuracy = model.evaluate(sensors_test, parcels_test)[1]# Evaluate your model's accuracy on the test data
print('Accuracy:', accuracy)# Print accuracy

# The history callback
h_callback = model.fit(X_train, y_train, epochs = 50,validation_data=(X_test, y_test))# Train your model and save its history
plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])# Plot train vs test loss during training
plot_accuracy(h_callback.history['acc'], h_callback.history['val_acc'])# Plot train vs test accuracy during training

# Early stopping your model
from keras.callbacks import EarlyStopping# Import the early stopping callback
monitor_val_acc = EarlyStopping(monitor='val_acc', patience=5)# Define a callback to monitor val_acc
model.fit(X_train, y_train,epochs=1000, validation_data=(X_test, y_test),callbacks=[monitor_val_acc])# Train your model using the early stopping callback

# A combination of callbacks
from keras.callbacks import EarlyStopping, ModelCheckpoint# Import the EarlyStopping and ModelCheckpoint callbacks
monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience = 3)# Early stop on validation accuracy
modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)# Save the best model as best_banknote_model.hdf5
h_callback = model.fit(X_train, y_train,epochs = 1000000000000,callbacks = [monitor_val_acc, modelCheckpoint],
                    validation_data = (X_test, y_test))# Fit your model for a stupid amount of epochs





# Improving Your Model Performance
# Learning the digits
model = Sequential()# Instantiate a Sequential model
model.add(Dense(16, input_shape = (64,), activation = 'relu'))# Input and hidden layer with input_shape, 16 neurons, and relu 
model.add(Dense(10, activation = 'softmax'))# Output layer with 10 neurons (one per digit) and softmax
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])# Compile your model
print(model.predict(X_train))# Test if your model is well assembled by predicting before training

# Is the model overfitting?
h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)# Train your model for 60 epochs, using X_test and y_test as validation data
plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])# Extract from the h_callback object loss and val_loss to plot the learning curve

# Do we need more data?
for size in training_sizes:  
    X_train_frac, y_train_frac = X_train[:size], y_train[:size]	# Get a fraction of training data (we only care about the training data)    
    model.set_weights(initial_weights)# Reset the model to the initial weights and train it on the new training data fraction
    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])    
    train_accs.append(model.evaluate(X_train, y_train)[1])# Evaluate and store both: the training data fraction and the complete test set results
    test_accs.append(model.evaluate(X_test, y_test)[1])    
plot_results(train_accs, test_accs)# Plot train vs test accuracies

# Comparing activation functions
activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']# Activation functions to try
activation_results = {}# Loop over the activation functions
for act in activations:
  model = get_model(act) # Get a new model with the current activation  
  h_callback = model.fit(X_train, y_train, epochs = 20, validation_data = (X_test, y_test),verbose=0)# Fit the model and store the history results
  activation_results[act] = h_callback

# Comparing activation functions II
val_loss= pd.DataFrame(val_loss_per_function)# Create a dataframe from val_loss_per_function
val_loss.plot()# Call plot on the dataframe
plt.show()
val_acc = pd.DataFrame(val_acc_per_function)# Create a dataframe from val_acc_per_function
val_acc.plot()# Call plot on the dataframe
plt.show()

# Changing batch sizes
model = get_model()# Get a fresh new model with get_model
model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0])# Fit your model for 5 epochs with a batch of size the training set
print("\n The accuracy when using the whole training set as batch-size was: ",
      model.evaluate(X_test, y_test)[1])

# Batch normalizing a familiar model
from keras.layers import BatchNormalization# Import batch normalization from keras layers
batchnorm_model = Sequential()# Build your deep network
batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))
batchnorm_model.add(BatchNormalization())
batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))
batchnorm_model.add(BatchNormalization())
batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))
batchnorm_model.add(BatchNormalization())
batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))
batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])# Compile your model with sgd

# Batch normalization effects
h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)# Train your standard model, storing its history callback
h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)# Train the batch normalized model you recently built, store its history callback
compare_histories_acc(h1_callback, h2_callback)# Call compare_histories_acc passing in both model histories

# Preparing a model for tuning
def create_model(learning_rate, activation):# Creates a model given an activation and learning rate  	
  	opt = Adam(lr = learning_rate) 	# Create an Adam optimizer with the given learning rate  	
  	model = Sequential()# Create your binary classification model  
  	model.add(Dense(128, input_shape = (30,), activation = activation))
  	model.add(Dense(256, activation = activation))
  	model.add(Dense(1, activation = 'sigmoid'))  	 	
  	model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])# Compile your model with your optimizer, loss, and metrics
  	return model

# Tuning the model parameters
from keras.wrappers.scikit_learn import KerasClassifier# Import KerasClassifier from keras scikit learn wrappers
model = KerasClassifier(build_fn = create_model)# Create a KerasClassifier
params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], # Define the parameters to try out
          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}
random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))# Create a randomize search cv object passing in the parameters to try
show_results()# Running random_search.fit(X,y) would start the search,but it takes too long! 

# Training with cross-validation
from keras.wrappers.scikit_learn import KerasClassifier# Import KerasClassifier from keras wrappers
model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, 
             batch_size = 128, verbose = 0)# Create a KerasClassifier
kfolds = cross_val_score(model, X, y, cv = 3)# Calculate the accuracy score for each fold
print('The mean accuracy was:', kfolds.mean())# Print the mean accuracy
print('With a standard deviation of:', kfolds.std())# Print the accuracy standard deviation





# Advanced Model Architectures
# It's a flow of tensors
import keras.backend as K# Import keras backend
inp = model.layers[0].input# Input tensor from the 1st layer of the model
out = model.layers[0].output# Output tensor from the 1st layer of the model
inp_to_out = K.function([inp], [out])# Define a function from inputs to outputs
print(inp_to_out([X_test]))# Print the results of passing X_test through the 1st layer

# Neural separation
for i in range(0, 21):
    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)# Train model for 1 epoch
    if i%4==0:     
      layer_output = inp_to_out([X_test])[0]# Get the output of the first layer            
      test_accuracy = model.evaluate(X_test, y_test)[1]# Evaluate model accuracy for this epoch             
      plot()# Plot 1st vs 2nd neuron output

# Building an autoencoder
autoencoder = Sequential()# Start with a sequential model
autoencoder.add(Dense(32, input_shape=(784, ), activation="relu"))# Add a dense layer with input the original image pixels and neurons the encoded representation
autoencoder.add(Dense(784, activation = "sigmoid"))# Add an output layer with as many neurons as the orginal image pixels
autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')# Compile your model with adadelta
autoencoder.summary()# Summarize your model structure

# De-noising like an autoencoder
encoder = Sequential()# Build your encoder by using the first layer of your autoencoder
encoder.add(autoencoder.layers[0])
encodings = encoder.predict(X_test_noise)# Encode the noisy images and show the encodings for your favorite number [0-9]
show_encodings(encodings, number = 1)
encoder = Sequential()# Build your encoder by using the first layer of your autoencoder
encoder.add(autoencoder.layers[0])
encodings = encoder.predict(X_test_noise)# Encode the noisy images and show the encodings for your favorite number [0-9]
show_encodings(encodings, number = 1)
decoded_imgs = autoencoder.predict(X_test_noise)# Predict on the noisy images with your autoencoder
compare_plot(X_test_noise, decoded_imgs)# Plot noisy vs decoded images

# Building a CNN model
from keras.layers import Conv2D,Flatten# Import the Conv2D and Flatten layers and instantiate model
model = Sequential()
model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))# Add a convolutional layer of 32 filters of size 3x3
model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))# Add a convolutional layer of 16 filters of size 3x3
model.add(Flatten())# Flatten the previous layer output
model.add(Dense(10, activation = 'softmax'))# Add as many outputs as classes with softmax activation

# Looking at convolutions
first_layer_output = model.layers[0].output# Obtain a reference to the outputs of the first layer
first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)# Build a model using the model input and the first layer output
activations = first_layer_model.predict(X_test)# Use this model to predict on X_test
axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')# Plot the first digit of X_test for the 15th filter
axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')# Do the same but for the 18th filter now
plt.show()

# Preparing your input image
from keras.preprocessing import image# Import image and preprocess_input
from keras.applications.resnet50 import preprocess_input
img = image.load_img(img_path, target_size=(224, 224))# Load the image with the right target size for your model
img_array = image.img_to_array(img)# Turn it into an array
img_expanded = np.expand_dims(img_array, axis = 0)# Expand the dimensions of the image, this is so that it fits the expected model input format
img_ready = preprocess_input(img_expanded)# Pre-process the img in the same way original images were

# Using a real world model
model = ResNet50(weights='imagenet')# Instantiate a ResNet50 model with 'imagenet' weights
preds = model.predict(img_ready)# Predict with ResNet50 on your already processed img
print('Predicted:', decode_predictions(preds, top=3)[0])# Decode the first 3 predictions

# Text prediction with LSTMs
words = text.split()# Split text into an array of words
sentences = []# Make sentences of 4 words each, moving one word at a time
for i in range(4, len(words)):
  sentences.append(' '.join(words[i-4:i]))
tokenizer = Tokenizer()# Instantiate a Tokenizer, then fit it on the sentences
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)# Turn sentences into a sequence of numbers
print("Sentences: \n {} \n Sequences: \n {}".format(sentences[:5],sequences[:5]))

# Build your LSTM model
from keras.layers import Embedding, LSTM, Dense# Import the Embedding, LSTM and Dense layer
model = Sequential()
model.add(Embedding(input_dim = 44, input_length = 3, output_dim = 8, ))# Add an Embedding layer with the right parameters
model.add(LSTM(32))# Add a 32 unit LSTM layer
model.add(Dense(32, activation='relu'))# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax
model.add(Dense(vocab_size, activation='softmax'))
model.summary()

# Decode your predictions
def predict_text(test_text, model = model):
  if len(test_text.split()) != 3:
    print('Text input should be 3 words!')
    return False 
  test_seq = tokenizer.texts_to_sequences([test_text])# Turn the test_text into a sequence of numbers
  test_seq = np.array(test_seq) 
  pred = model.predict(test_seq).argmax(axis = 1)[0] # Use the model passed as a parameter to predict the next word 
  return tokenizer.index_word[pred]# Return the word that maps to the prediction
